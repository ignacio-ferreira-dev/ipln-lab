#!/usr/bin/env python3

ACTIVATION1 = 'relu'
ACTIVATION2 = 'relu'
ACTIVATION3 = 'relu'
BATCH_SIZE = 32
DROPOUT1 = 0.102013383622566957
DROPOUT2 = 0.108507892153196690
EPOCHS = 25
KERNEL_INITIALIZER1 = 'glorot_uniform'
KERNEL_INITIALIZER2 = 'glorot_uniform'
LOSS = 'binary_crossentropy'
MAX_WORDS_PER_TWEET = 30
OPTIMIZER = 'Adam'
RECURRENT_DROPOUT1 = 0.10818661117430111
RECURRENT_DROPOUT2 = 0.10046190552052199
TOTAL_UNITS = 50
